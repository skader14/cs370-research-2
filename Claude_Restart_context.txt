# CFR-RL Latency-Aware Extension - Restart Context

## Project Summary

**Goal:** Extend CFR-RL (Zhang et al., arXiv:2004.11986) to measure and optimize for latency, not just Maximum Link Utilization (MLU).

**Research Question:** Does MLU-optimized routing inadvertently hurt latency? Can we do better with a latency-aware objective?

**Status:** Training complete, CloudSimSDN simulations run, initial results collected.

---

## LATEST RESULTS

### Experiment 3: Heavy Workload (132 packets processed, ~146s simulation)

| Metric | MLU-only | Latency-aware | Difference | Winner |
|--------|----------|---------------|------------|--------|
| Mean queuing | 455.55ms | 455.96ms | +0.41ms | TIE |
| P90 queuing | 1153ms | 1150ms | -2.75ms | TIE |
| P95 queuing | 3591ms | 3591ms | 0 | TIE |
| Max queuing | 7500ms | 7500ms | 0 | TIE |
| Mean path length | 4.45 | 4.75 | +0.30 hops | MLU |
| Packets >100ms queue | 22 (16.7%) | 23 (17.4%) | +1 | MLU |

**Critical finding:** The TOP 10 WORST PACKETS ARE IDENTICAL between both models!

| Rank | Flow | Queuing | Route |
|------|------|---------|-------|
| 1 | flow_84 | 7500ms | vm_7→vm_8 |
| 2 | flow_50 | 7327ms | vm_4→vm_7 |
| 3 | flow_109 | 5368ms | vm_9→vm_11 |
| 4 | flow_130 | 5315ms | vm_11→vm_9 |

**Root cause:** The latency_weight changes HOW flows are routed, but not WHICH flows are selected for optimization. The same problematic flows (84, 50, 109, 130) cause congestion regardless.

**Conclusion:** Must add latency features to flow selection, and consider training with CloudSim for real latency feedback.

### Experiment 2: Light Workload After Bug Fix (51 packets)

| Metric | MLU-only | Latency-aware | Winner |
|--------|----------|---------------|--------|
| Mean queuing | 12.81ms | 14.06ms | MLU-only |
| Max queuing | 86.93ms | 100.00ms | MLU-only |

Latency-aware performed slightly worse due to longer paths.

---

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────┐
│  PYTHON (Training) - COMPLETE                                │
│  - abilene_trainer_latency.py: Trains RL policy             │
│  - abilene_lp_solver_latency.py: LP with latency_weight     │
│  - Outputs: best_abilene_lw0.0.pt, best_abilene_lw0.3.pt   │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│  JAVA (CloudSimSDN Simulation) - IN PROGRESS                 │
│  - CFRRLExample.java: Main simulation runner                 │
│  - LatencyCollector.java: NEW - records per-packet latency  │
│  - SDNDatacenter.java: MODIFIED - calls LatencyCollector    │
│  - NetworkOperatingSystem.java: MODIFIED - exposes channels │
└─────────────────────────────────────────────────────────────┘
```

---

## Key Concepts

### Latency Weight (LP Objective Modification)
- `latency_weight=0.0`: Pure MLU minimization (original Zhang paper)
- `latency_weight=0.3`: Trades some MLU for lower queuing delay
- Objective: `minimize(U + latency_weight * avg_utilization)`

### Latency Measurement Formula
```
queuing_delay = serve_time - propagation_delay - transmission_delay
where:
  serve_time = finish_time - start_time
  propagation = Channel.getTotalLatency() (physical distance)
  transmission = packet_size / bandwidth
```

### Training Process (REINFORCE)
- No fixed dataset - generates random traffic each iteration
- Policy network scores all 132 flows, samples K=8 to optimize
- LP solver computes optimal routing for selected flows
- Reward = 1/MLU (lower utilization = higher reward)

---

## Training Results Summary

Both models trained successfully:

| Model | Best Eval (iter 500) | Final Eval |
|-------|---------------------|------------|
| MLU-only (lw=0.0) | CFR-RL beats Top-K by 2.1% | CFR-RL beats Top-K by 5.4% |
| Latency-aware (lw=0.3) | CFR-RL beats Top-K by 2.1% | CFR-RL beats Top-K by 5.4% |

**Key observation:** Similar MLU performance, but real latency difference will appear in CloudSimSDN simulation.

---

## Files Modified for Latency Extension

### Java (CloudSimSDN)

1. **LatencyCollector.java** (NEW)
   - Location: `org/cloudbus/cloudsim/sdn/rl/LatencyCollector.java`
   - Purpose: Singleton that collects per-packet latency metrics
   - Key method: `recordPacketCompletion(flowId, packetId, startTime, finishTime, propagationDelay, packetSize, bandwidth, pathLength, srcNode, dstNode)`

2. **NetworkOperatingSystem.java** (MODIFIED)
   - Added: `pkt.setChannelInfo()` call in `processCompletePackets()` to store channel info before removal
   - (findChannel() method no longer needed for latency recording)

3. **SDNDatacenter.java** (MODIFIED)
   - In `processPacketCompleted()`: Uses `pkt.getChannelXxx()` to get latency info
   - Added fields: `latencyRecordingEnabled`, `packetsProcessedCount`, `latencyLogFrequency`

4. **Packet.java** (MODIFIED)
   - Added: `channelTotalLatency`, `channelBandwidth`, `channelPathLength` fields
   - Added: `setChannelInfo()` and getters for channel info

5. **CFRRLExampleLatency.java** (NEW)
   - Runner for latency experiments
   - All outputs go to `outputs/` folder

### Python (Training)

6. **abilene_lp_solver_latency.py** (MODIFIED)
   - Added `latency_weight` parameter to `solve()` method
   - New objective: `minimize(U + latency_weight * avg_utilization)`

7. **abilene_trainer_latency.py** (MODIFIED)
   - `TrainingConfig.latency_weight` field
   - Passes `latency_weight` to solver in `train_step()` and `evaluate()`

8. **agent.py** (MODIFIED)
   - Easy model switching via `MODEL_PATH` and `EXPERIMENT_NAME` at top
   - Outputs to `outputs/` folder

9. **compare_latency_results.py** (NEW)
   - Compares two latency CSV files
   - Prints queuing delay, serve time, path length statistics

---

## Output Folder Structure

All outputs go to the `outputs/` folder for easy organization:

```
outputs/
├── cfrrl_debug_mlu_only.log           # Java debug log (MLU experiment)
├── cfrrl_debug_latency_aware.log      # Java debug log (latency experiment)
├── cfrrl_agent_mlu_only.log           # Python agent log
├── cfrrl_agent_latency_aware.log      # Python agent log
├── latency_results_mlu_only.csv       # Per-packet latency data
└── latency_results_latency_aware.csv  # Per-packet latency data
```

### How to Switch Experiments

**Step 1: Edit agent.py (Python)**
```python
# At top of file, uncomment ONE:
MODEL_PATH = "best_abilene_lw0.0.pt"   # MLU-only
# MODEL_PATH = "best_abilene_lw0.3.pt"   # Latency-aware

EXPERIMENT_NAME = "mlu_only"           # Must match Java
# EXPERIMENT_NAME = "latency_aware"
```

**Step 2: Edit CFRRLExampleLatency.java (Java)**
```java
private static final String EXPERIMENT_NAME = "mlu_only";  // Must match Python
// private static final String EXPERIMENT_NAME = "latency_aware";
```

**Step 3: Run**
```bash
mvn exec:java -Dexec.mainClass="org.cloudbus.cloudsim.sdn.example.CFRRLExampleLatency"
```

---

## Abilene Topology

- 12 nodes (Seattle, Sunnyvale, LosAngeles, Denver, KansasCity, Houston, Chicago, Indianapolis, Atlanta, Washington, NewYork, Jacksonville)
- 30 links with latencies 2-12ms
- 132 flows (12×11 directed pairs)
- K=8 critical flows selected by RL policy

---

## Current State & Next Steps

### Completed
- [x] LatencyCollector.java created and integrated
- [x] SDNDatacenter.java modified to record latency
- [x] NetworkOperatingSystem.java modified to store channel info on packet
- [x] Packet.java modified with channel info fields
- [x] LP solver modified with latency_weight
- [x] Trainer modified to pass latency_weight
- [x] Trained MLU-only model (best_abilene_lw0.0.pt)
- [x] Trained latency-aware model (best_abilene_lw0.3.pt)
- [x] Fixed Channel lookup bug
- [x] Created heavy workload (500 packets, 136s)
- [x] Ran heavy workload experiments
- [x] Identified root cause: same flows cause congestion regardless of model

### Next Steps (In Order)
1. [ ] **Add LP-derived features to flow selection** (no CloudSim needed)
   - path_congestion_score: max link utilization on flow's path
   - shared_bottleneck_count: flows sharing same bottleneck
   - min_propagation_delay: physical latency of shortest path
   
2. [ ] **Retrain with new features** (Python-only, ~1 hour)
   - Modify abilene_trainer_latency.py to compute new features
   - Train new model: best_abilene_latency_features.pt
   
3. [ ] **Test new model in CloudSim** (compare to previous)

4. [ ] **Implement CloudSim-in-the-loop training** (research contribution)
   - Create training harness that runs CloudSim episodes
   - Reward based on actual queuing delay
   - Fine-tune pre-trained model

---

## Files Needed to Continue

Upload these files if starting a new chat:

1. **Java files** (to verify integration):
   - LatencyCollector.java
   - SDNDatacenter.java (modified version)
   - NetworkOperatingSystem.java (modified version)
   - CFRRLExampleLatency.java

2. **Python files** (to verify training):
   - abilene_lp_solver_latency.py
   - abilene_trainer_latency.py
   - agent.py (with outputs folder support)

3. **Config files**:
   - abilene-physical.json
   - abilene-virtual.json
   - abilene-workload.csv (light - 51 packets)
   - abilene-workload-heavy.csv (heavy - 500 packets)

4. **Trained models**:
   - best_abilene_lw0.0.pt (MLU-only)
   - best_abilene_lw0.3.pt (latency-aware)

5. **Results** (if available):
   - outputs/latency_results_mlu_only.csv
   - outputs/latency_results_latency_aware.csv

6. **Analysis tools**:
   - compare_latency_results.py

---

## Prompt to Continue

Paste this to start a new conversation:

```
I'm extending CFR-RL (Zhang et al., arXiv:2004.11986) to optimize for latency.

CURRENT STATE:
- Training complete with latency_weight in LP objective
- Heavy workload experiments run (132 packets, 146s)
- KEY FINDING: Same flows cause worst queuing in BOTH models!
  - Top offenders: flow_84 (7.5s), flow_50 (7.3s), flow_109 (5.4s)
  - latency_weight changes routing but not which flows are optimized
  - Root cause: flow SELECTION doesn't consider latency

NEXT STEPS:
1. Add LP-derived features to flow selection:
   - path_congestion_score (max utilization on path)
   - shared_bottleneck_count (flows sharing bottleneck)
   - min_propagation_delay (physical latency)
2. Retrain with new features
3. Eventually: CloudSim-in-the-loop training for real latency feedback

ARCHITECTURE:
- Python: RL policy selects K=8 critical flows based on features
- LP solver: routes selected flows optimally
- CloudSim: packet-level simulation, measures actual latency

FILES:
- abilene_trainer_latency.py: training loop
- abilene_lp_solver_latency.py: LP with latency_weight
- agent.py: inference for CloudSim
- CFRRLExampleLatency.java: simulation runner
- compare_latency_results.py: analysis tool

I need help with: [describe current issue]
```

---

## Reference: Zhang Paper Not Required

The key concepts from the Zhang paper are already encoded in the implementation:
- REINFORCE algorithm for policy gradient
- Flow selection as the RL action
- LP solver for optimal routing given selected flows
- MLU minimization as the original objective

The latency extension is our novel contribution.

---

## Bugs Fixed During Development

1. **torch.load() PyTorch 2.6 compatibility**: Add `weights_only=False`
2. **Final eval not loading best model**: Changed from `except: pass` to proper error handling
3. **KeyError 'temperature' on skipped iterations**: Use `self.current_temperature` instead of `metrics['temperature']`

---

## Known Issues (To Fix)

### 1. Channel lookup returns null - **FIXED ✓**

**Solution applied:** Store channel info on Packet in `processCompletePackets()` before channel is removed.

**Files modified:**
1. `Packet.java` - Added 3 fields + getters/setters
2. `NetworkOperatingSystem.java` - Calls `pkt.setChannelInfo()` in `processCompletePackets()`
3. `SDNDatacenter.java` - Uses `pkt.getChannelXxx()` instead of `findChannel()`

**Result:** Now properly measures propagation_delay, transmission_delay, queuing_delay.

---

## Workloads

### Light Workload: abilene-workload.csv (original)
- 51 packets over ~20 seconds
- Good for quick tests, insufficient for congestion differentiation

### Heavy Workload: abilene-workload-heavy.csv
- 500 packets over ~136 seconds
- 94 GB total data, avg packet 189 MB
- Includes traffic bursts and hot spots to create congestion
- Use for meaningful latency comparison

To switch workloads, edit `CFRRLExampleLatency.java`:
```java
protected static String[] workloadFiles = {
    "dataset-abilene/abilene-workload-heavy.csv"  // Heavy workload
    // "dataset-abilene/abilene-workload.csv"     // Light workload
};
```

---

## Analysis Tools

### compare_latency_results.py

Compares two latency CSV files and prints statistics:

```bash
python compare_latency_results.py outputs/latency_results_mlu_only.csv outputs/latency_results_latency_aware.csv
```

Output includes:
- Queuing delay (mean, median, P95, P99, max)
- Serve time (end-to-end latency)
- Path length differences
- Packets with >1ms queuing delay

---

## Research Extension Ideas

### 1. Latency Features in Flow Selection (High Impact) - NEXT STEP

**Problem identified:** The top 10 worst-queuing packets are IDENTICAL between MLU-only and latency-aware models. The LP routing changes don't matter because the SAME flows cause congestion.

**Solution:** Add latency-aware features to flow selection so the RL policy learns to identify latency-sensitive flows.

**Feature types by data source:**

| Type | Source | Examples |
|------|--------|----------|
| Static | Topology JSON | propagation_delay, link_capacity |
| LP-Derived | After LP solve | expected_link_utilization, shared_bottleneck_count |
| Simulation-Derived | CloudSim only | actual_queuing_delay, dropped_packets |

**Proposed features (can implement without CloudSim):**
```python
# Current features (4)
features = [demand, num_paths, path_length, bottleneck_capacity]

# Extended features (7) - LP-derived, no CloudSim needed
features = [
    demand,                      # Keep: larger flows need optimization
    num_paths,                   # Keep: routing flexibility
    path_length,                 # Keep: more hops = more delay potential
    bottleneck_capacity,         # Keep: capacity constraint
    min_propagation_delay,       # NEW: physical latency of shortest path
    path_congestion_score,       # NEW: max utilization on flow's path (from LP)
    shared_bottleneck_count,     # NEW: how many flows share my bottleneck link
]
```

### 2. Simulator-in-the-Loop Training (Research Contribution)

**Why needed:** Python-only training uses avg_utilization as proxy for queuing delay. Heavy workload experiments show this proxy fails - same flows cause problems regardless of routing.

**Architecture: Hybrid Training**

```
PHASE 1: Pre-training with LP (Fast, ~50 min)
├── 3000 iterations, ~1s each
├── Python-only, LP proxy for latency
├── Gets policy to reasonable starting point
└── Reward = 1/MLU

PHASE 2: Fine-tuning with CloudSim (Accurate, ~2-4 hours)
├── 200-500 episodes, 30-60s each
├── Each episode:
│   ├── Generate random workload (200 packets, 60s)
│   ├── Run CloudSim with current policy
│   ├── Read latency_results.csv
│   ├── Compute reward from ACTUAL queuing delays
│   └── Update policy via REINFORCE
└── Reward = -mean_queuing_delay (or -P95)
```

**Implementation options:**

*Option A: Batch Episodes (Simpler)*
- Python script controls everything
- Runs CloudSim as subprocess for each episode
- Reads CSV after each run
- ~30s per episode = 500 episodes in 4 hours

*Option B: Interactive (Extend current architecture)*
- Longer RL_INTERVAL (30-60s) during training
- CloudSim sends latency stats to Python each interval
- Python computes reward and updates policy
- Requires protocol extension for reward feedback

**Reward function options:**
```python
reward = -mean_queuing_delay           # Simple average
reward = -np.percentile(queuing, 95)   # Tail latency focus
reward = -mean - 0.5 * std             # Penalize variance
reward = -(queuing > 100ms).sum()      # Count of bad packets
```

**State augmentation for Phase 2:**
```python
state = {
    'flows': [...],                    # Flow features
    'recent_link_congestion': [...],   # Which links congested last interval
    'recent_flow_queuing': [...],      # Per-flow queuing stats
}
```

### 3. Dynamic latency_weight

Adjust latency_weight based on network state:
- Low congestion → latency_weight = 0 (pure MLU)
- High congestion → latency_weight = 0.5 (spread load)

(Lower priority than features and CloudSim training)